{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (108815, 4) (108815,)\n",
      "Validation set shape: (23317, 4) (23317,)\n",
      "Test set shape: (23318, 4) (23318,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data from the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"data_clean.csv\")\n",
    "\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = data[['flair', 'score', 'post-comment_length', 'lemmatized-comment']]\n",
    "y = data['numerical-label']\n",
    "\n",
    "# Split the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to download [GoogleNews-vectors-negative300](https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Furkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Furkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the data from the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"data_clean.csv\")\n",
    "\n",
    "data.dropna(subset=['lemmatized-comment'], inplace=True)\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "word2vec_model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def generate_word_embeddings(tokens):\n",
    "    word_embeddings = []\n",
    "    for word in tokens:\n",
    "        if word in word2vec_model:\n",
    "            word_embedding = word2vec_model[word]\n",
    "            word_embeddings.append(word_embedding)\n",
    "    return word_embeddings\n",
    "\n",
    "data['tokens'] = data['lemmatized-comment'].apply(lambda x: x.split())\n",
    "\n",
    "# Generate word embeddings for each document\n",
    "data['word-embeddings'] = data['tokens'].apply(generate_word_embeddings)\n",
    "\n",
    "# Optionally, you can also generate document embeddings by averaging word embeddings or using other aggregation methods\n",
    "\n",
    "# Save the processed data with word embeddings\n",
    "data.to_csv(\"data_with_word_embeddings.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
