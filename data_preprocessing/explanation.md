# Explanation for the work

On data_prep, we are basically cleaning, merging and encoding the data. 

On lemmatizer, we are tokenizing and lemmatizing the words to catch better sequences between words.

On embedder, use it to your own discretion at the moment since it took my 40mb csv to 32 gbs and it was impossible to load it afterwards since RAM limitations.
