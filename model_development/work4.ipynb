{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data_clean.csv')\n",
    "\n",
    "# Make sure your text data does not contain NaN values\n",
    "df['lemmatized-comment'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['lemmatized-comment'], inplace=True)\n",
    "\n",
    "# Split the data into training and validation datasets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['lemmatized-comment'], df['numerical-label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['numerical-label'])\n",
    "\n",
    "# We'll further split the validation set into validation and test datasets\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode the text\n",
    "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "validation_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "validation_labels = torch.tensor(val_labels.tolist())\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "validation_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_labels = torch.tensor(test_labels.tolist())\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=16)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Furkan\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6757, Loss: 2.93\n",
      "Batch: 250/6757, Loss: 2.38\n",
      "Batch: 500/6757, Loss: 2.41\n",
      "Batch: 750/6757, Loss: 2.54\n",
      "Batch: 1000/6757, Loss: 2.31\n",
      "Batch: 1250/6757, Loss: 2.37\n",
      "Batch: 1500/6757, Loss: 2.37\n",
      "Batch: 1750/6757, Loss: 2.27\n",
      "Batch: 2000/6757, Loss: 2.73\n",
      "Batch: 2250/6757, Loss: 2.39\n",
      "Batch: 2500/6757, Loss: 2.45\n",
      "Batch: 2750/6757, Loss: 2.81\n",
      "Batch: 3000/6757, Loss: 2.33\n",
      "Batch: 3250/6757, Loss: 2.19\n",
      "Batch: 3500/6757, Loss: 2.31\n",
      "Batch: 3750/6757, Loss: 2.11\n",
      "Batch: 4000/6757, Loss: 1.82\n",
      "Batch: 4250/6757, Loss: 2.19\n",
      "Batch: 4500/6757, Loss: 2.65\n",
      "Batch: 4750/6757, Loss: 2.31\n",
      "Batch: 5000/6757, Loss: 2.35\n",
      "Batch: 5250/6757, Loss: 2.46\n",
      "Batch: 5500/6757, Loss: 2.20\n",
      "Batch: 5750/6757, Loss: 2.03\n",
      "Batch: 6000/6757, Loss: 2.21\n",
      "Batch: 6250/6757, Loss: 2.03\n",
      "Batch: 6500/6757, Loss: 2.20\n",
      "Batch: 6750/6757, Loss: 2.14\n",
      "Average training loss: 2.39\n",
      "Validation Accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|█▋        | 1/6 [57:11<4:45:57, 3431.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6757, Loss: 2.63\n",
      "Batch: 250/6757, Loss: 2.34\n",
      "Batch: 500/6757, Loss: 2.17\n",
      "Batch: 750/6757, Loss: 2.34\n",
      "Batch: 1000/6757, Loss: 2.40\n",
      "Batch: 1250/6757, Loss: 1.96\n",
      "Batch: 1500/6757, Loss: 2.51\n",
      "Batch: 1750/6757, Loss: 2.33\n",
      "Batch: 2000/6757, Loss: 2.45\n",
      "Batch: 2250/6757, Loss: 2.10\n",
      "Batch: 2500/6757, Loss: 2.17\n",
      "Batch: 2750/6757, Loss: 2.49\n",
      "Batch: 3000/6757, Loss: 2.38\n",
      "Batch: 3250/6757, Loss: 2.08\n",
      "Batch: 3500/6757, Loss: 2.11\n",
      "Batch: 3750/6757, Loss: 2.04\n",
      "Batch: 4000/6757, Loss: 2.27\n",
      "Batch: 4250/6757, Loss: 2.08\n",
      "Batch: 4500/6757, Loss: 2.71\n",
      "Batch: 4750/6757, Loss: 2.50\n",
      "Batch: 5000/6757, Loss: 2.16\n",
      "Batch: 5250/6757, Loss: 2.27\n",
      "Batch: 5500/6757, Loss: 2.16\n",
      "Batch: 5750/6757, Loss: 2.27\n",
      "Batch: 6000/6757, Loss: 2.33\n",
      "Batch: 6250/6757, Loss: 2.24\n",
      "Batch: 6500/6757, Loss: 2.19\n",
      "Batch: 6750/6757, Loss: 2.04\n",
      "Average training loss: 2.23\n",
      "Validation Accuracy: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 2/6 [1:54:28<3:48:58, 3434.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6757, Loss: 2.23\n",
      "Batch: 250/6757, Loss: 2.16\n",
      "Batch: 500/6757, Loss: 1.97\n",
      "Batch: 750/6757, Loss: 2.19\n",
      "Batch: 1000/6757, Loss: 1.97\n",
      "Batch: 1250/6757, Loss: 2.27\n",
      "Batch: 1500/6757, Loss: 2.14\n",
      "Batch: 1750/6757, Loss: 2.03\n",
      "Batch: 2000/6757, Loss: 2.29\n",
      "Batch: 2250/6757, Loss: 2.05\n",
      "Batch: 2500/6757, Loss: 2.07\n",
      "Batch: 2750/6757, Loss: 1.90\n",
      "Batch: 3000/6757, Loss: 2.79\n",
      "Batch: 3250/6757, Loss: 2.26\n",
      "Batch: 3500/6757, Loss: 2.01\n",
      "Batch: 3750/6757, Loss: 2.09\n",
      "Batch: 4000/6757, Loss: 1.95\n",
      "Batch: 4250/6757, Loss: 2.18\n",
      "Batch: 4500/6757, Loss: 2.15\n",
      "Batch: 4750/6757, Loss: 2.00\n",
      "Batch: 5000/6757, Loss: 2.31\n",
      "Batch: 5250/6757, Loss: 2.07\n",
      "Batch: 5500/6757, Loss: 2.36\n",
      "Batch: 5750/6757, Loss: 2.00\n",
      "Batch: 6000/6757, Loss: 2.05\n",
      "Batch: 6250/6757, Loss: 2.16\n",
      "Batch: 6500/6757, Loss: 1.93\n",
      "Batch: 6750/6757, Loss: 1.82\n",
      "Average training loss: 2.13\n",
      "Validation Accuracy: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 3/6 [2:51:45<2:51:47, 3435.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6757, Loss: 2.23\n",
      "Batch: 250/6757, Loss: 2.17\n",
      "Batch: 500/6757, Loss: 2.15\n",
      "Batch: 750/6757, Loss: 2.19\n",
      "Batch: 1000/6757, Loss: 1.95\n",
      "Batch: 1250/6757, Loss: 1.94\n",
      "Batch: 1500/6757, Loss: 2.13\n",
      "Batch: 1750/6757, Loss: 2.37\n",
      "Batch: 2000/6757, Loss: 2.03\n",
      "Batch: 2250/6757, Loss: 2.40\n",
      "Batch: 2500/6757, Loss: 2.02\n",
      "Batch: 2750/6757, Loss: 2.01\n",
      "Batch: 3000/6757, Loss: 2.50\n",
      "Batch: 3250/6757, Loss: 2.14\n",
      "Batch: 3500/6757, Loss: 2.18\n",
      "Batch: 3750/6757, Loss: 2.66\n",
      "Batch: 4000/6757, Loss: 2.23\n",
      "Batch: 4250/6757, Loss: 1.91\n",
      "Batch: 4500/6757, Loss: 2.35\n",
      "Batch: 4750/6757, Loss: 2.67\n",
      "Batch: 5000/6757, Loss: 2.14\n",
      "Batch: 5250/6757, Loss: 1.81\n",
      "Batch: 5500/6757, Loss: 2.17\n",
      "Batch: 5750/6757, Loss: 2.33\n",
      "Batch: 6000/6757, Loss: 1.87\n",
      "Batch: 6250/6757, Loss: 2.26\n",
      "Batch: 6500/6757, Loss: 2.18\n",
      "Batch: 6750/6757, Loss: 2.31\n",
      "Average training loss: 2.08\n",
      "Validation Accuracy: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 4/6 [3:49:07<1:54:36, 3438.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6757, Loss: 1.64\n",
      "Batch: 250/6757, Loss: 2.26\n",
      "Batch: 500/6757, Loss: 2.14\n",
      "Batch: 750/6757, Loss: 2.46\n",
      "Batch: 1000/6757, Loss: 2.18\n",
      "Batch: 1250/6757, Loss: 2.19\n",
      "Batch: 1500/6757, Loss: 1.86\n",
      "Batch: 1750/6757, Loss: 2.09\n",
      "Batch: 2000/6757, Loss: 2.32\n",
      "Batch: 2250/6757, Loss: 2.10\n",
      "Batch: 2500/6757, Loss: 1.90\n",
      "Batch: 2750/6757, Loss: 1.90\n",
      "Batch: 3000/6757, Loss: 2.18\n",
      "Batch: 3250/6757, Loss: 2.13\n",
      "Batch: 3500/6757, Loss: 2.16\n",
      "Batch: 3750/6757, Loss: 1.79\n",
      "Batch: 4000/6757, Loss: 1.88\n",
      "Batch: 4250/6757, Loss: 1.86\n",
      "Batch: 4500/6757, Loss: 2.14\n",
      "Batch: 4750/6757, Loss: 2.37\n",
      "Batch: 5000/6757, Loss: 1.89\n",
      "Batch: 5250/6757, Loss: 1.94\n",
      "Batch: 5500/6757, Loss: 2.02\n",
      "Batch: 5750/6757, Loss: 1.93\n",
      "Batch: 6000/6757, Loss: 2.28\n",
      "Batch: 6250/6757, Loss: 2.10\n",
      "Batch: 6500/6757, Loss: 1.93\n",
      "Batch: 6750/6757, Loss: 2.42\n",
      "Average training loss: 2.09\n",
      "Validation Accuracy: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%|████████▎ | 5/6 [4:46:44<57:25, 3445.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6757, Loss: 1.72\n",
      "Batch: 250/6757, Loss: 1.95\n",
      "Batch: 500/6757, Loss: 1.91\n",
      "Batch: 750/6757, Loss: 1.86\n",
      "Batch: 1000/6757, Loss: 2.32\n",
      "Batch: 1250/6757, Loss: 1.67\n",
      "Batch: 1500/6757, Loss: 2.11\n",
      "Batch: 1750/6757, Loss: 2.19\n",
      "Batch: 2000/6757, Loss: 2.63\n",
      "Batch: 2250/6757, Loss: 2.22\n",
      "Batch: 2500/6757, Loss: 1.80\n",
      "Batch: 2750/6757, Loss: 1.83\n",
      "Batch: 3000/6757, Loss: 2.25\n",
      "Batch: 3250/6757, Loss: 2.43\n",
      "Batch: 3500/6757, Loss: 2.46\n",
      "Batch: 3750/6757, Loss: 2.09\n",
      "Batch: 4000/6757, Loss: 2.09\n",
      "Batch: 4250/6757, Loss: 1.87\n",
      "Batch: 4500/6757, Loss: 1.60\n",
      "Batch: 4750/6757, Loss: 2.17\n",
      "Batch: 5000/6757, Loss: 1.74\n",
      "Batch: 5250/6757, Loss: 2.42\n",
      "Batch: 5500/6757, Loss: 1.88\n",
      "Batch: 5750/6757, Loss: 1.95\n",
      "Batch: 6000/6757, Loss: 2.21\n",
      "Batch: 6250/6757, Loss: 2.05\n",
      "Batch: 6500/6757, Loss: 2.12\n",
      "Batch: 6750/6757, Loss: 1.97\n",
      "Average training loss: 2.09\n",
      "Validation Accuracy: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 6/6 [5:44:08<00:00, 3441.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.27\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 16) # assuming you have 16 types of MBTI\n",
    "model.to(device)\n",
    "\n",
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*3) # assuming you want 3 epochs\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 6\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if step % 250 == 0:\n",
    "            print(\"Batch: {0}/{1}, Loss: {2:.2f}\".format(step, len(train_dataloader), loss.item()))\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    torch.save(model.state_dict(), f'checkpoint2_{_}.pt') # Save the model checkpoint\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'bert_model2.pt')\n",
    "\n",
    "def evaluate_test_set(test_dataloader):\n",
    "    # Put model in evaluation mode to evaluate loss on the test set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Test Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# Call the function to evaluate the test set\n",
    "evaluate_test_set(test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
