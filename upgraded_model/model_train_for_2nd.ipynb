{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Read your data\n",
    "df = pd.read_csv('data2.csv')\n",
    "\n",
    "# Clean and preprocess\n",
    "df['lemmatized-comment'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['lemmatized-comment'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['lemmatized-comment'], df['numerical-label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.25, \n",
    "                                                                    stratify=df['numerical-label'])\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "validation_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "validation_labels = torch.tensor(val_labels.tolist())\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "validation_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_labels = torch.tensor(test_labels.tolist())\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=16)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Furkan\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/83949, Loss: 2.01\n",
      "Batch: 250/83949, Loss: 1.64\n",
      "Batch: 500/83949, Loss: 2.06\n",
      "Batch: 750/83949, Loss: 1.95\n",
      "Batch: 1000/83949, Loss: 2.32\n",
      "Batch: 1250/83949, Loss: 1.46\n",
      "Batch: 1500/83949, Loss: 1.71\n",
      "Batch: 1750/83949, Loss: 2.01\n",
      "Batch: 2000/83949, Loss: 1.26\n",
      "Batch: 2250/83949, Loss: 1.82\n",
      "Batch: 2500/83949, Loss: 1.84\n",
      "Batch: 2750/83949, Loss: 1.70\n",
      "Batch: 3000/83949, Loss: 2.05\n",
      "Batch: 3250/83949, Loss: 1.64\n",
      "Batch: 3500/83949, Loss: 1.71\n",
      "Batch: 3750/83949, Loss: 1.64\n",
      "Batch: 4000/83949, Loss: 1.35\n",
      "Batch: 4250/83949, Loss: 1.48\n",
      "Batch: 4500/83949, Loss: 1.73\n",
      "Batch: 4750/83949, Loss: 1.46\n",
      "Batch: 5000/83949, Loss: 1.80\n",
      "Batch: 5250/83949, Loss: 1.72\n",
      "Batch: 5500/83949, Loss: 1.92\n",
      "Batch: 5750/83949, Loss: 1.68\n",
      "Batch: 6000/83949, Loss: 1.91\n",
      "Batch: 6250/83949, Loss: 2.08\n",
      "Batch: 6500/83949, Loss: 1.91\n",
      "Batch: 6750/83949, Loss: 2.74\n",
      "Batch: 7000/83949, Loss: 1.99\n",
      "Batch: 7250/83949, Loss: 1.56\n",
      "Batch: 7500/83949, Loss: 1.66\n",
      "Batch: 7750/83949, Loss: 1.63\n",
      "Batch: 8000/83949, Loss: 2.15\n",
      "Batch: 8250/83949, Loss: 1.77\n",
      "Batch: 8500/83949, Loss: 1.53\n",
      "Batch: 8750/83949, Loss: 1.85\n",
      "Batch: 9000/83949, Loss: 1.30\n",
      "Batch: 9250/83949, Loss: 1.99\n",
      "Batch: 9500/83949, Loss: 2.17\n",
      "Batch: 9750/83949, Loss: 1.98\n",
      "Batch: 10000/83949, Loss: 1.96\n",
      "Batch: 10250/83949, Loss: 2.07\n",
      "Batch: 10500/83949, Loss: 1.53\n",
      "Batch: 10750/83949, Loss: 1.89\n",
      "Batch: 11000/83949, Loss: 1.85\n",
      "Batch: 11250/83949, Loss: 1.55\n",
      "Batch: 11500/83949, Loss: 1.70\n",
      "Batch: 11750/83949, Loss: 1.77\n",
      "Batch: 12000/83949, Loss: 1.59\n",
      "Batch: 12250/83949, Loss: 1.86\n",
      "Batch: 12500/83949, Loss: 2.21\n",
      "Batch: 12750/83949, Loss: 1.96\n",
      "Batch: 13000/83949, Loss: 1.44\n",
      "Batch: 13250/83949, Loss: 2.42\n",
      "Batch: 13500/83949, Loss: 1.57\n",
      "Batch: 13750/83949, Loss: 1.92\n",
      "Batch: 14000/83949, Loss: 1.75\n",
      "Batch: 14250/83949, Loss: 2.50\n",
      "Batch: 14500/83949, Loss: 2.08\n",
      "Batch: 14750/83949, Loss: 1.68\n",
      "Batch: 15000/83949, Loss: 2.06\n",
      "Batch: 15250/83949, Loss: 1.83\n",
      "Batch: 15500/83949, Loss: 1.72\n",
      "Batch: 15750/83949, Loss: 1.83\n",
      "Batch: 16000/83949, Loss: 2.14\n",
      "Batch: 16250/83949, Loss: 2.23\n",
      "Batch: 16500/83949, Loss: 1.69\n",
      "Batch: 16750/83949, Loss: 1.59\n",
      "Batch: 17000/83949, Loss: 1.71\n",
      "Batch: 17250/83949, Loss: 1.91\n",
      "Batch: 17500/83949, Loss: 1.61\n",
      "Batch: 17750/83949, Loss: 1.77\n",
      "Batch: 18000/83949, Loss: 1.87\n",
      "Batch: 18250/83949, Loss: 1.42\n",
      "Batch: 18500/83949, Loss: 1.32\n",
      "Batch: 18750/83949, Loss: 2.07\n",
      "Batch: 19000/83949, Loss: 2.10\n",
      "Batch: 19250/83949, Loss: 1.65\n",
      "Batch: 19500/83949, Loss: 1.80\n",
      "Batch: 19750/83949, Loss: 2.01\n",
      "Batch: 20000/83949, Loss: 2.04\n",
      "Batch: 20250/83949, Loss: 1.74\n",
      "Batch: 20500/83949, Loss: 1.66\n",
      "Batch: 20750/83949, Loss: 2.06\n",
      "Batch: 21000/83949, Loss: 1.72\n",
      "Batch: 21250/83949, Loss: 1.96\n",
      "Batch: 21500/83949, Loss: 1.79\n",
      "Batch: 21750/83949, Loss: 1.73\n",
      "Batch: 22000/83949, Loss: 1.95\n",
      "Batch: 22250/83949, Loss: 1.60\n",
      "Batch: 22500/83949, Loss: 1.84\n",
      "Batch: 22750/83949, Loss: 1.75\n",
      "Batch: 23000/83949, Loss: 1.36\n",
      "Batch: 23250/83949, Loss: 1.68\n",
      "Batch: 23500/83949, Loss: 2.05\n",
      "Batch: 23750/83949, Loss: 2.07\n",
      "Batch: 24000/83949, Loss: 1.79\n",
      "Batch: 24250/83949, Loss: 1.93\n",
      "Batch: 24500/83949, Loss: 1.90\n",
      "Batch: 24750/83949, Loss: 1.74\n",
      "Batch: 25000/83949, Loss: 1.77\n",
      "Batch: 25250/83949, Loss: 1.52\n",
      "Batch: 25500/83949, Loss: 1.60\n",
      "Batch: 25750/83949, Loss: 1.80\n",
      "Batch: 26000/83949, Loss: 1.54\n",
      "Batch: 26250/83949, Loss: 1.62\n",
      "Batch: 26500/83949, Loss: 1.78\n",
      "Batch: 26750/83949, Loss: 2.02\n",
      "Batch: 27000/83949, Loss: 1.70\n",
      "Batch: 27250/83949, Loss: 1.68\n",
      "Batch: 27500/83949, Loss: 1.87\n",
      "Batch: 27750/83949, Loss: 2.14\n",
      "Batch: 28000/83949, Loss: 1.80\n",
      "Batch: 28250/83949, Loss: 2.37\n",
      "Batch: 28500/83949, Loss: 1.78\n",
      "Batch: 28750/83949, Loss: 2.21\n",
      "Batch: 29000/83949, Loss: 2.38\n",
      "Batch: 29250/83949, Loss: 1.83\n",
      "Batch: 29500/83949, Loss: 1.43\n",
      "Batch: 29750/83949, Loss: 2.44\n",
      "Batch: 30000/83949, Loss: 1.53\n",
      "Batch: 30250/83949, Loss: 1.94\n",
      "Batch: 30500/83949, Loss: 1.61\n",
      "Batch: 30750/83949, Loss: 1.73\n",
      "Batch: 31000/83949, Loss: 2.03\n",
      "Batch: 31250/83949, Loss: 2.42\n",
      "Batch: 31500/83949, Loss: 1.41\n",
      "Batch: 31750/83949, Loss: 1.89\n",
      "Batch: 32000/83949, Loss: 1.85\n",
      "Batch: 32250/83949, Loss: 2.15\n",
      "Batch: 32500/83949, Loss: 1.92\n",
      "Batch: 32750/83949, Loss: 1.52\n",
      "Batch: 33000/83949, Loss: 1.87\n",
      "Batch: 33250/83949, Loss: 1.80\n",
      "Batch: 33500/83949, Loss: 2.02\n",
      "Batch: 33750/83949, Loss: 1.50\n",
      "Batch: 34000/83949, Loss: 1.65\n",
      "Batch: 34250/83949, Loss: 2.16\n",
      "Batch: 34500/83949, Loss: 1.96\n",
      "Batch: 34750/83949, Loss: 1.62\n",
      "Batch: 35000/83949, Loss: 1.84\n",
      "Batch: 35250/83949, Loss: 1.82\n",
      "Batch: 35500/83949, Loss: 1.72\n",
      "Batch: 35750/83949, Loss: 1.51\n",
      "Batch: 36000/83949, Loss: 1.60\n",
      "Batch: 36250/83949, Loss: 1.53\n",
      "Batch: 36500/83949, Loss: 1.69\n",
      "Batch: 36750/83949, Loss: 1.37\n",
      "Batch: 37000/83949, Loss: 1.63\n",
      "Batch: 37250/83949, Loss: 1.87\n",
      "Batch: 37500/83949, Loss: 1.33\n",
      "Batch: 37750/83949, Loss: 1.70\n",
      "Batch: 38000/83949, Loss: 1.81\n",
      "Batch: 38250/83949, Loss: 2.23\n",
      "Batch: 38500/83949, Loss: 1.88\n",
      "Batch: 38750/83949, Loss: 1.38\n",
      "Batch: 39000/83949, Loss: 1.85\n",
      "Batch: 39250/83949, Loss: 1.76\n",
      "Batch: 39500/83949, Loss: 1.61\n",
      "Batch: 39750/83949, Loss: 1.36\n",
      "Batch: 40000/83949, Loss: 1.90\n",
      "Batch: 40250/83949, Loss: 1.55\n",
      "Batch: 40500/83949, Loss: 1.80\n",
      "Batch: 40750/83949, Loss: 2.15\n",
      "Batch: 41000/83949, Loss: 2.07\n",
      "Batch: 41250/83949, Loss: 1.82\n",
      "Batch: 41500/83949, Loss: 2.45\n",
      "Batch: 41750/83949, Loss: 1.46\n",
      "Batch: 42000/83949, Loss: 1.96\n",
      "Batch: 42250/83949, Loss: 1.61\n",
      "Batch: 42500/83949, Loss: 1.89\n",
      "Batch: 42750/83949, Loss: 1.86\n",
      "Batch: 43000/83949, Loss: 1.87\n",
      "Batch: 43250/83949, Loss: 1.84\n",
      "Batch: 43500/83949, Loss: 1.89\n",
      "Batch: 43750/83949, Loss: 1.63\n",
      "Batch: 44000/83949, Loss: 2.15\n",
      "Batch: 44250/83949, Loss: 1.69\n",
      "Batch: 44500/83949, Loss: 1.26\n",
      "Batch: 44750/83949, Loss: 1.77\n",
      "Batch: 45000/83949, Loss: 1.32\n",
      "Batch: 45250/83949, Loss: 1.94\n",
      "Batch: 45500/83949, Loss: 1.73\n",
      "Batch: 45750/83949, Loss: 1.76\n",
      "Batch: 46000/83949, Loss: 1.27\n",
      "Batch: 46250/83949, Loss: 2.03\n",
      "Batch: 46500/83949, Loss: 1.88\n",
      "Batch: 46750/83949, Loss: 1.83\n",
      "Batch: 47000/83949, Loss: 1.28\n",
      "Batch: 47250/83949, Loss: 1.70\n",
      "Batch: 47500/83949, Loss: 1.73\n",
      "Batch: 47750/83949, Loss: 1.98\n",
      "Batch: 48000/83949, Loss: 1.68\n",
      "Batch: 48250/83949, Loss: 1.89\n",
      "Batch: 48500/83949, Loss: 1.25\n",
      "Batch: 48750/83949, Loss: 1.35\n",
      "Batch: 49000/83949, Loss: 1.56\n",
      "Batch: 49250/83949, Loss: 2.05\n",
      "Batch: 49500/83949, Loss: 1.76\n",
      "Batch: 49750/83949, Loss: 1.26\n",
      "Batch: 50000/83949, Loss: 1.23\n",
      "Batch: 50250/83949, Loss: 2.59\n",
      "Batch: 50500/83949, Loss: 2.04\n",
      "Batch: 50750/83949, Loss: 1.65\n",
      "Batch: 51000/83949, Loss: 2.12\n",
      "Batch: 51250/83949, Loss: 1.54\n",
      "Batch: 51500/83949, Loss: 1.31\n",
      "Batch: 51750/83949, Loss: 1.33\n",
      "Batch: 52000/83949, Loss: 1.49\n",
      "Batch: 52250/83949, Loss: 2.07\n",
      "Batch: 52500/83949, Loss: 1.73\n",
      "Batch: 52750/83949, Loss: 1.69\n",
      "Batch: 53000/83949, Loss: 1.79\n",
      "Batch: 53250/83949, Loss: 1.94\n",
      "Batch: 53500/83949, Loss: 1.98\n",
      "Batch: 53750/83949, Loss: 1.77\n",
      "Batch: 54000/83949, Loss: 2.06\n",
      "Batch: 54250/83949, Loss: 2.07\n",
      "Batch: 54500/83949, Loss: 1.92\n",
      "Batch: 54750/83949, Loss: 1.51\n",
      "Batch: 55000/83949, Loss: 2.13\n",
      "Batch: 55250/83949, Loss: 1.52\n",
      "Batch: 55500/83949, Loss: 2.30\n",
      "Batch: 55750/83949, Loss: 2.05\n",
      "Batch: 56000/83949, Loss: 1.74\n",
      "Batch: 56250/83949, Loss: 1.83\n",
      "Batch: 56500/83949, Loss: 1.91\n",
      "Batch: 56750/83949, Loss: 2.34\n",
      "Batch: 57000/83949, Loss: 1.75\n",
      "Batch: 57250/83949, Loss: 1.78\n",
      "Batch: 57500/83949, Loss: 1.74\n",
      "Batch: 57750/83949, Loss: 1.69\n",
      "Batch: 58000/83949, Loss: 1.79\n",
      "Batch: 58250/83949, Loss: 1.51\n",
      "Batch: 58500/83949, Loss: 2.01\n",
      "Batch: 58750/83949, Loss: 2.28\n",
      "Batch: 59000/83949, Loss: 1.70\n",
      "Batch: 59250/83949, Loss: 1.36\n",
      "Batch: 59500/83949, Loss: 2.02\n",
      "Batch: 59750/83949, Loss: 1.31\n",
      "Batch: 60000/83949, Loss: 1.89\n",
      "Batch: 60250/83949, Loss: 1.86\n",
      "Batch: 60500/83949, Loss: 1.85\n",
      "Batch: 60750/83949, Loss: 1.48\n",
      "Batch: 61000/83949, Loss: 2.43\n",
      "Batch: 61250/83949, Loss: 1.58\n",
      "Batch: 61500/83949, Loss: 2.04\n",
      "Batch: 61750/83949, Loss: 2.14\n",
      "Batch: 62000/83949, Loss: 1.10\n",
      "Batch: 62250/83949, Loss: 1.54\n",
      "Batch: 62500/83949, Loss: 2.18\n",
      "Batch: 62750/83949, Loss: 1.78\n",
      "Batch: 63000/83949, Loss: 2.15\n",
      "Batch: 63250/83949, Loss: 1.81\n",
      "Batch: 63500/83949, Loss: 2.29\n",
      "Batch: 63750/83949, Loss: 2.06\n",
      "Batch: 64000/83949, Loss: 1.97\n",
      "Batch: 64250/83949, Loss: 1.61\n",
      "Batch: 64500/83949, Loss: 1.62\n",
      "Batch: 64750/83949, Loss: 1.61\n",
      "Batch: 65000/83949, Loss: 1.95\n",
      "Batch: 65250/83949, Loss: 2.12\n",
      "Batch: 65500/83949, Loss: 1.94\n",
      "Batch: 65750/83949, Loss: 1.82\n",
      "Batch: 66000/83949, Loss: 2.46\n",
      "Batch: 66250/83949, Loss: 1.57\n",
      "Batch: 66500/83949, Loss: 1.95\n",
      "Batch: 66750/83949, Loss: 1.86\n",
      "Batch: 67000/83949, Loss: 1.61\n",
      "Batch: 67250/83949, Loss: 2.61\n",
      "Batch: 67500/83949, Loss: 1.88\n",
      "Batch: 67750/83949, Loss: 1.85\n",
      "Batch: 68000/83949, Loss: 1.55\n",
      "Batch: 68250/83949, Loss: 2.11\n",
      "Batch: 68500/83949, Loss: 1.42\n",
      "Batch: 68750/83949, Loss: 1.42\n",
      "Batch: 69000/83949, Loss: 1.52\n",
      "Batch: 69250/83949, Loss: 1.68\n",
      "Batch: 69500/83949, Loss: 2.00\n",
      "Batch: 69750/83949, Loss: 1.74\n",
      "Batch: 70000/83949, Loss: 2.06\n",
      "Batch: 70250/83949, Loss: 1.52\n",
      "Batch: 70500/83949, Loss: 1.41\n",
      "Batch: 70750/83949, Loss: 2.23\n",
      "Batch: 71000/83949, Loss: 1.68\n",
      "Batch: 71250/83949, Loss: 1.97\n",
      "Batch: 71500/83949, Loss: 1.85\n",
      "Batch: 71750/83949, Loss: 2.27\n",
      "Batch: 72000/83949, Loss: 1.74\n",
      "Batch: 72250/83949, Loss: 1.55\n",
      "Batch: 72500/83949, Loss: 1.95\n",
      "Batch: 72750/83949, Loss: 1.54\n",
      "Batch: 73000/83949, Loss: 1.98\n",
      "Batch: 73250/83949, Loss: 1.58\n",
      "Batch: 73500/83949, Loss: 2.02\n",
      "Batch: 73750/83949, Loss: 1.80\n",
      "Batch: 74000/83949, Loss: 1.36\n",
      "Batch: 74250/83949, Loss: 1.91\n",
      "Batch: 74500/83949, Loss: 2.58\n",
      "Batch: 74750/83949, Loss: 1.84\n",
      "Batch: 75000/83949, Loss: 1.97\n",
      "Batch: 75250/83949, Loss: 2.30\n",
      "Batch: 75500/83949, Loss: 1.55\n",
      "Batch: 75750/83949, Loss: 2.23\n",
      "Batch: 76000/83949, Loss: 1.64\n",
      "Batch: 76250/83949, Loss: 1.87\n",
      "Batch: 76500/83949, Loss: 1.55\n",
      "Batch: 76750/83949, Loss: 2.21\n",
      "Batch: 77000/83949, Loss: 1.90\n",
      "Batch: 77250/83949, Loss: 1.21\n",
      "Batch: 77500/83949, Loss: 1.66\n",
      "Batch: 77750/83949, Loss: 1.46\n",
      "Batch: 78000/83949, Loss: 2.03\n",
      "Batch: 78250/83949, Loss: 1.95\n",
      "Batch: 78500/83949, Loss: 1.63\n",
      "Batch: 78750/83949, Loss: 1.75\n",
      "Batch: 79000/83949, Loss: 2.44\n",
      "Batch: 79250/83949, Loss: 1.40\n",
      "Batch: 79500/83949, Loss: 1.16\n",
      "Batch: 79750/83949, Loss: 1.38\n",
      "Batch: 80000/83949, Loss: 1.29\n",
      "Batch: 80250/83949, Loss: 1.87\n",
      "Batch: 80500/83949, Loss: 1.94\n",
      "Batch: 80750/83949, Loss: 1.35\n",
      "Batch: 81000/83949, Loss: 1.81\n",
      "Batch: 81250/83949, Loss: 1.54\n",
      "Batch: 81500/83949, Loss: 1.75\n",
      "Batch: 81750/83949, Loss: 2.15\n",
      "Batch: 82000/83949, Loss: 1.52\n",
      "Batch: 82250/83949, Loss: 1.11\n",
      "Batch: 82500/83949, Loss: 1.70\n",
      "Batch: 82750/83949, Loss: 1.77\n",
      "Batch: 83000/83949, Loss: 1.94\n",
      "Batch: 83250/83949, Loss: 1.68\n",
      "Batch: 83500/83949, Loss: 1.31\n",
      "Batch: 83750/83949, Loss: 1.45\n",
      "Average training loss: 1.81\n",
      "Validation Accuracy: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [11:39:15<34:57:45, 41955.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/83949, Loss: 2.30\n",
      "Batch: 250/83949, Loss: 2.07\n",
      "Batch: 500/83949, Loss: 1.95\n",
      "Batch: 750/83949, Loss: 1.96\n",
      "Batch: 1000/83949, Loss: 2.17\n",
      "Batch: 1250/83949, Loss: 1.08\n",
      "Batch: 1500/83949, Loss: 2.14\n",
      "Batch: 1750/83949, Loss: 1.71\n",
      "Batch: 2000/83949, Loss: 1.69\n",
      "Batch: 2250/83949, Loss: 1.30\n",
      "Batch: 2500/83949, Loss: 1.71\n",
      "Batch: 2750/83949, Loss: 2.18\n",
      "Batch: 3000/83949, Loss: 1.36\n",
      "Batch: 3250/83949, Loss: 1.90\n",
      "Batch: 3500/83949, Loss: 1.52\n",
      "Batch: 3750/83949, Loss: 1.48\n",
      "Batch: 4000/83949, Loss: 1.70\n",
      "Batch: 4250/83949, Loss: 1.87\n",
      "Batch: 4500/83949, Loss: 1.09\n",
      "Batch: 4750/83949, Loss: 1.99\n",
      "Batch: 5000/83949, Loss: 1.60\n",
      "Batch: 5250/83949, Loss: 2.47\n",
      "Batch: 5500/83949, Loss: 1.52\n",
      "Batch: 5750/83949, Loss: 1.75\n",
      "Batch: 6000/83949, Loss: 1.73\n",
      "Batch: 6250/83949, Loss: 1.76\n",
      "Batch: 6500/83949, Loss: 1.21\n",
      "Batch: 6750/83949, Loss: 1.33\n",
      "Batch: 7000/83949, Loss: 1.97\n",
      "Batch: 7250/83949, Loss: 1.75\n",
      "Batch: 7500/83949, Loss: 1.91\n",
      "Batch: 7750/83949, Loss: 1.49\n",
      "Batch: 8000/83949, Loss: 1.67\n",
      "Batch: 8250/83949, Loss: 1.71\n",
      "Batch: 8500/83949, Loss: 1.37\n",
      "Batch: 8750/83949, Loss: 1.54\n",
      "Batch: 9000/83949, Loss: 1.86\n",
      "Batch: 9250/83949, Loss: 1.74\n",
      "Batch: 9500/83949, Loss: 2.46\n",
      "Batch: 9750/83949, Loss: 1.72\n",
      "Batch: 10000/83949, Loss: 1.79\n",
      "Batch: 10250/83949, Loss: 1.79\n",
      "Batch: 10500/83949, Loss: 1.50\n",
      "Batch: 10750/83949, Loss: 1.56\n",
      "Batch: 11000/83949, Loss: 1.89\n",
      "Batch: 11250/83949, Loss: 1.73\n",
      "Batch: 11500/83949, Loss: 1.74\n",
      "Batch: 11750/83949, Loss: 1.78\n",
      "Batch: 12000/83949, Loss: 1.42\n",
      "Batch: 12250/83949, Loss: 1.53\n",
      "Batch: 12500/83949, Loss: 1.37\n",
      "Batch: 12750/83949, Loss: 1.86\n",
      "Batch: 13000/83949, Loss: 1.57\n",
      "Batch: 13250/83949, Loss: 1.96\n",
      "Batch: 13500/83949, Loss: 2.10\n",
      "Batch: 13750/83949, Loss: 1.46\n",
      "Batch: 14000/83949, Loss: 1.70\n",
      "Batch: 14250/83949, Loss: 1.38\n",
      "Batch: 14500/83949, Loss: 1.97\n",
      "Batch: 14750/83949, Loss: 2.23\n",
      "Batch: 15000/83949, Loss: 1.85\n",
      "Batch: 15250/83949, Loss: 1.88\n",
      "Batch: 15500/83949, Loss: 1.35\n",
      "Batch: 15750/83949, Loss: 1.38\n",
      "Batch: 16000/83949, Loss: 1.98\n",
      "Batch: 16250/83949, Loss: 1.89\n",
      "Batch: 16500/83949, Loss: 1.49\n",
      "Batch: 16750/83949, Loss: 1.55\n",
      "Batch: 17000/83949, Loss: 1.75\n",
      "Batch: 17250/83949, Loss: 1.59\n",
      "Batch: 17500/83949, Loss: 1.96\n",
      "Batch: 17750/83949, Loss: 2.45\n",
      "Batch: 18000/83949, Loss: 1.75\n",
      "Batch: 18250/83949, Loss: 1.57\n",
      "Batch: 18500/83949, Loss: 1.99\n",
      "Batch: 18750/83949, Loss: 2.01\n",
      "Batch: 19000/83949, Loss: 1.90\n",
      "Batch: 19250/83949, Loss: 2.36\n",
      "Batch: 19500/83949, Loss: 1.58\n",
      "Batch: 19750/83949, Loss: 1.81\n",
      "Batch: 20000/83949, Loss: 1.47\n",
      "Batch: 20250/83949, Loss: 1.77\n",
      "Batch: 20500/83949, Loss: 1.40\n",
      "Batch: 20750/83949, Loss: 1.54\n",
      "Batch: 21000/83949, Loss: 1.65\n",
      "Batch: 21250/83949, Loss: 1.85\n",
      "Batch: 21500/83949, Loss: 2.05\n",
      "Batch: 21750/83949, Loss: 1.65\n",
      "Batch: 22000/83949, Loss: 1.52\n",
      "Batch: 22250/83949, Loss: 1.60\n",
      "Batch: 22500/83949, Loss: 1.58\n",
      "Batch: 22750/83949, Loss: 1.66\n",
      "Batch: 23000/83949, Loss: 2.32\n",
      "Batch: 23250/83949, Loss: 1.72\n",
      "Batch: 23500/83949, Loss: 2.39\n",
      "Batch: 23750/83949, Loss: 1.59\n",
      "Batch: 24000/83949, Loss: 1.21\n",
      "Batch: 24250/83949, Loss: 2.06\n",
      "Batch: 24500/83949, Loss: 1.72\n",
      "Batch: 24750/83949, Loss: 1.63\n",
      "Batch: 25000/83949, Loss: 1.78\n",
      "Batch: 25250/83949, Loss: 2.09\n",
      "Batch: 25500/83949, Loss: 2.38\n",
      "Batch: 25750/83949, Loss: 1.84\n",
      "Batch: 26000/83949, Loss: 1.56\n",
      "Batch: 26250/83949, Loss: 1.85\n",
      "Batch: 26500/83949, Loss: 2.13\n",
      "Batch: 26750/83949, Loss: 1.71\n",
      "Batch: 27000/83949, Loss: 1.56\n",
      "Batch: 27250/83949, Loss: 1.46\n",
      "Batch: 27500/83949, Loss: 1.45\n",
      "Batch: 27750/83949, Loss: 2.04\n",
      "Batch: 28000/83949, Loss: 1.88\n",
      "Batch: 28250/83949, Loss: 1.10\n",
      "Batch: 28500/83949, Loss: 1.83\n",
      "Batch: 28750/83949, Loss: 1.50\n",
      "Batch: 29000/83949, Loss: 1.79\n",
      "Batch: 29250/83949, Loss: 1.86\n",
      "Batch: 29500/83949, Loss: 1.45\n",
      "Batch: 29750/83949, Loss: 1.93\n",
      "Batch: 30000/83949, Loss: 1.54\n",
      "Batch: 30250/83949, Loss: 1.74\n",
      "Batch: 30500/83949, Loss: 1.75\n",
      "Batch: 30750/83949, Loss: 1.44\n",
      "Batch: 31000/83949, Loss: 1.54\n",
      "Batch: 31250/83949, Loss: 1.53\n",
      "Batch: 31500/83949, Loss: 1.41\n",
      "Batch: 31750/83949, Loss: 1.91\n",
      "Batch: 32000/83949, Loss: 1.91\n",
      "Batch: 32250/83949, Loss: 2.45\n",
      "Batch: 32500/83949, Loss: 1.27\n",
      "Batch: 32750/83949, Loss: 1.68\n",
      "Batch: 33000/83949, Loss: 2.27\n",
      "Batch: 33250/83949, Loss: 1.56\n",
      "Batch: 33500/83949, Loss: 2.28\n",
      "Batch: 33750/83949, Loss: 1.58\n",
      "Batch: 34000/83949, Loss: 1.98\n",
      "Batch: 34250/83949, Loss: 2.05\n",
      "Batch: 34500/83949, Loss: 1.62\n",
      "Batch: 34750/83949, Loss: 1.98\n",
      "Batch: 35000/83949, Loss: 2.14\n",
      "Batch: 35250/83949, Loss: 1.69\n",
      "Batch: 35500/83949, Loss: 1.27\n",
      "Batch: 35750/83949, Loss: 1.54\n",
      "Batch: 36000/83949, Loss: 1.69\n",
      "Batch: 36250/83949, Loss: 1.25\n",
      "Batch: 36500/83949, Loss: 1.45\n",
      "Batch: 36750/83949, Loss: 1.74\n",
      "Batch: 37000/83949, Loss: 1.67\n",
      "Batch: 37250/83949, Loss: 1.68\n",
      "Batch: 37500/83949, Loss: 1.27\n",
      "Batch: 37750/83949, Loss: 2.27\n",
      "Batch: 38000/83949, Loss: 1.47\n",
      "Batch: 38250/83949, Loss: 1.50\n",
      "Batch: 38500/83949, Loss: 1.80\n",
      "Batch: 38750/83949, Loss: 1.24\n",
      "Batch: 39000/83949, Loss: 1.56\n",
      "Batch: 39250/83949, Loss: 1.45\n",
      "Batch: 39500/83949, Loss: 1.51\n",
      "Batch: 39750/83949, Loss: 1.68\n",
      "Batch: 40000/83949, Loss: 1.92\n",
      "Batch: 40250/83949, Loss: 1.66\n",
      "Batch: 40500/83949, Loss: 1.72\n",
      "Batch: 40750/83949, Loss: 1.45\n",
      "Batch: 41000/83949, Loss: 1.89\n",
      "Batch: 41250/83949, Loss: 1.89\n",
      "Batch: 41500/83949, Loss: 1.56\n",
      "Batch: 41750/83949, Loss: 1.69\n",
      "Batch: 42000/83949, Loss: 1.47\n",
      "Batch: 42250/83949, Loss: 1.68\n",
      "Batch: 42500/83949, Loss: 2.56\n",
      "Batch: 42750/83949, Loss: 1.45\n",
      "Batch: 43000/83949, Loss: 2.08\n",
      "Batch: 43250/83949, Loss: 1.56\n",
      "Batch: 43500/83949, Loss: 1.31\n",
      "Batch: 43750/83949, Loss: 1.79\n",
      "Batch: 44000/83949, Loss: 1.73\n",
      "Batch: 44250/83949, Loss: 1.64\n",
      "Batch: 44500/83949, Loss: 1.42\n",
      "Batch: 44750/83949, Loss: 1.61\n",
      "Batch: 45000/83949, Loss: 1.58\n",
      "Batch: 45250/83949, Loss: 1.83\n",
      "Batch: 45500/83949, Loss: 1.61\n",
      "Batch: 45750/83949, Loss: 1.73\n",
      "Batch: 46000/83949, Loss: 2.06\n",
      "Batch: 46250/83949, Loss: 1.46\n",
      "Batch: 46500/83949, Loss: 1.59\n",
      "Batch: 46750/83949, Loss: 2.08\n",
      "Batch: 47000/83949, Loss: 1.57\n",
      "Batch: 47250/83949, Loss: 1.60\n",
      "Batch: 47500/83949, Loss: 1.48\n",
      "Batch: 47750/83949, Loss: 1.97\n",
      "Batch: 48000/83949, Loss: 2.23\n",
      "Batch: 48250/83949, Loss: 1.90\n",
      "Batch: 48500/83949, Loss: 1.87\n",
      "Batch: 48750/83949, Loss: 1.48\n",
      "Batch: 49000/83949, Loss: 1.40\n",
      "Batch: 49250/83949, Loss: 1.57\n",
      "Batch: 49500/83949, Loss: 1.67\n",
      "Batch: 49750/83949, Loss: 1.90\n",
      "Batch: 50000/83949, Loss: 1.91\n",
      "Batch: 50250/83949, Loss: 1.16\n",
      "Batch: 50500/83949, Loss: 2.02\n",
      "Batch: 50750/83949, Loss: 1.99\n",
      "Batch: 51000/83949, Loss: 2.00\n",
      "Batch: 51250/83949, Loss: 1.76\n",
      "Batch: 51500/83949, Loss: 1.35\n",
      "Batch: 51750/83949, Loss: 1.90\n",
      "Batch: 52000/83949, Loss: 1.72\n",
      "Batch: 52250/83949, Loss: 1.60\n",
      "Batch: 52500/83949, Loss: 1.51\n",
      "Batch: 52750/83949, Loss: 2.04\n",
      "Batch: 53000/83949, Loss: 1.99\n",
      "Batch: 53250/83949, Loss: 1.61\n",
      "Batch: 53500/83949, Loss: 1.13\n",
      "Batch: 53750/83949, Loss: 2.12\n",
      "Batch: 54000/83949, Loss: 2.11\n",
      "Batch: 54250/83949, Loss: 1.91\n",
      "Batch: 54500/83949, Loss: 2.31\n",
      "Batch: 54750/83949, Loss: 1.99\n",
      "Batch: 55000/83949, Loss: 1.96\n",
      "Batch: 55250/83949, Loss: 2.34\n",
      "Batch: 55500/83949, Loss: 1.82\n",
      "Batch: 55750/83949, Loss: 1.91\n",
      "Batch: 56000/83949, Loss: 1.88\n",
      "Batch: 56250/83949, Loss: 1.60\n",
      "Batch: 56500/83949, Loss: 1.93\n",
      "Batch: 56750/83949, Loss: 1.87\n",
      "Batch: 57000/83949, Loss: 1.23\n",
      "Batch: 57250/83949, Loss: 1.46\n",
      "Batch: 57500/83949, Loss: 2.06\n",
      "Batch: 57750/83949, Loss: 2.06\n",
      "Batch: 58000/83949, Loss: 1.93\n",
      "Batch: 58250/83949, Loss: 1.64\n",
      "Batch: 58500/83949, Loss: 1.63\n",
      "Batch: 58750/83949, Loss: 1.17\n",
      "Batch: 59000/83949, Loss: 1.66\n",
      "Batch: 59250/83949, Loss: 1.96\n",
      "Batch: 59500/83949, Loss: 1.77\n",
      "Batch: 59750/83949, Loss: 2.22\n",
      "Batch: 60000/83949, Loss: 1.52\n",
      "Batch: 60250/83949, Loss: 1.40\n",
      "Batch: 60500/83949, Loss: 1.75\n",
      "Batch: 60750/83949, Loss: 1.61\n",
      "Batch: 61000/83949, Loss: 1.70\n",
      "Batch: 61250/83949, Loss: 1.97\n",
      "Batch: 61500/83949, Loss: 1.77\n",
      "Batch: 61750/83949, Loss: 1.70\n",
      "Batch: 62000/83949, Loss: 1.46\n",
      "Batch: 62250/83949, Loss: 1.67\n",
      "Batch: 62500/83949, Loss: 1.40\n",
      "Batch: 62750/83949, Loss: 2.41\n",
      "Batch: 63000/83949, Loss: 1.16\n",
      "Batch: 63250/83949, Loss: 1.73\n",
      "Batch: 63500/83949, Loss: 1.94\n",
      "Batch: 63750/83949, Loss: 1.69\n",
      "Batch: 64000/83949, Loss: 1.48\n",
      "Batch: 64250/83949, Loss: 1.75\n",
      "Batch: 64500/83949, Loss: 1.70\n",
      "Batch: 64750/83949, Loss: 1.93\n",
      "Batch: 65000/83949, Loss: 1.31\n",
      "Batch: 65250/83949, Loss: 1.72\n",
      "Batch: 65500/83949, Loss: 1.17\n",
      "Batch: 65750/83949, Loss: 1.32\n",
      "Batch: 66000/83949, Loss: 1.85\n",
      "Batch: 66250/83949, Loss: 2.23\n",
      "Batch: 66500/83949, Loss: 1.87\n",
      "Batch: 66750/83949, Loss: 1.70\n",
      "Batch: 67000/83949, Loss: 1.48\n",
      "Batch: 67250/83949, Loss: 1.51\n",
      "Batch: 67500/83949, Loss: 1.18\n",
      "Batch: 67750/83949, Loss: 1.77\n",
      "Batch: 68000/83949, Loss: 2.11\n",
      "Batch: 68250/83949, Loss: 1.51\n",
      "Batch: 68500/83949, Loss: 2.09\n",
      "Batch: 68750/83949, Loss: 1.27\n",
      "Batch: 69000/83949, Loss: 2.14\n",
      "Batch: 69250/83949, Loss: 1.47\n",
      "Batch: 69500/83949, Loss: 1.81\n",
      "Batch: 69750/83949, Loss: 1.96\n",
      "Batch: 70000/83949, Loss: 1.81\n",
      "Batch: 70250/83949, Loss: 1.70\n",
      "Batch: 70500/83949, Loss: 2.25\n",
      "Batch: 70750/83949, Loss: 1.81\n",
      "Batch: 71000/83949, Loss: 1.24\n",
      "Batch: 71250/83949, Loss: 1.65\n",
      "Batch: 71500/83949, Loss: 1.68\n",
      "Batch: 71750/83949, Loss: 1.76\n",
      "Batch: 72000/83949, Loss: 1.94\n",
      "Batch: 72250/83949, Loss: 1.31\n",
      "Batch: 72500/83949, Loss: 1.74\n",
      "Batch: 72750/83949, Loss: 1.79\n",
      "Batch: 73000/83949, Loss: 1.66\n",
      "Batch: 73250/83949, Loss: 1.64\n",
      "Batch: 73500/83949, Loss: 2.56\n",
      "Batch: 73750/83949, Loss: 1.56\n",
      "Batch: 74000/83949, Loss: 1.88\n",
      "Batch: 74250/83949, Loss: 1.75\n",
      "Batch: 74500/83949, Loss: 1.79\n",
      "Batch: 74750/83949, Loss: 1.79\n",
      "Batch: 75000/83949, Loss: 2.00\n",
      "Batch: 75250/83949, Loss: 1.71\n",
      "Batch: 75500/83949, Loss: 1.55\n",
      "Batch: 75750/83949, Loss: 1.60\n",
      "Batch: 76000/83949, Loss: 1.59\n",
      "Batch: 76250/83949, Loss: 1.62\n",
      "Batch: 76500/83949, Loss: 1.76\n",
      "Batch: 76750/83949, Loss: 1.55\n",
      "Batch: 77000/83949, Loss: 1.78\n",
      "Batch: 77250/83949, Loss: 2.12\n",
      "Batch: 77500/83949, Loss: 1.78\n",
      "Batch: 77750/83949, Loss: 2.07\n",
      "Batch: 78000/83949, Loss: 1.62\n",
      "Batch: 78250/83949, Loss: 1.74\n",
      "Batch: 78500/83949, Loss: 1.65\n",
      "Batch: 78750/83949, Loss: 1.70\n",
      "Batch: 79000/83949, Loss: 1.90\n",
      "Batch: 79250/83949, Loss: 1.41\n",
      "Batch: 79500/83949, Loss: 1.46\n",
      "Batch: 79750/83949, Loss: 1.31\n",
      "Batch: 80000/83949, Loss: 1.66\n",
      "Batch: 80250/83949, Loss: 1.68\n",
      "Batch: 80500/83949, Loss: 1.67\n",
      "Batch: 80750/83949, Loss: 1.80\n",
      "Batch: 81000/83949, Loss: 1.62\n",
      "Batch: 81250/83949, Loss: 1.78\n",
      "Batch: 81500/83949, Loss: 2.03\n",
      "Batch: 81750/83949, Loss: 1.74\n",
      "Batch: 82000/83949, Loss: 2.07\n",
      "Batch: 82250/83949, Loss: 1.49\n",
      "Batch: 82500/83949, Loss: 1.86\n",
      "Batch: 82750/83949, Loss: 2.13\n",
      "Batch: 83000/83949, Loss: 1.90\n",
      "Batch: 83250/83949, Loss: 1.30\n",
      "Batch: 83500/83949, Loss: 1.63\n",
      "Batch: 83750/83949, Loss: 1.72\n",
      "Average training loss: 1.71\n",
      "Validation Accuracy: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [23:16:53<23:16:36, 41898.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/83949, Loss: 1.63\n",
      "Batch: 250/83949, Loss: 1.49\n",
      "Batch: 500/83949, Loss: 2.02\n",
      "Batch: 750/83949, Loss: 1.23\n",
      "Batch: 1000/83949, Loss: 1.66\n",
      "Batch: 1250/83949, Loss: 1.49\n",
      "Batch: 1500/83949, Loss: 1.49\n",
      "Batch: 1750/83949, Loss: 2.04\n",
      "Batch: 2000/83949, Loss: 1.74\n",
      "Batch: 2250/83949, Loss: 1.71\n",
      "Batch: 2500/83949, Loss: 1.68\n",
      "Batch: 2750/83949, Loss: 1.55\n",
      "Batch: 3000/83949, Loss: 2.09\n",
      "Batch: 3250/83949, Loss: 1.63\n",
      "Batch: 3500/83949, Loss: 1.53\n",
      "Batch: 3750/83949, Loss: 1.11\n",
      "Batch: 4000/83949, Loss: 1.59\n",
      "Batch: 4250/83949, Loss: 1.31\n",
      "Batch: 4500/83949, Loss: 1.67\n",
      "Batch: 4750/83949, Loss: 2.05\n",
      "Batch: 5000/83949, Loss: 1.98\n",
      "Batch: 5250/83949, Loss: 1.53\n",
      "Batch: 5500/83949, Loss: 1.99\n",
      "Batch: 5750/83949, Loss: 2.37\n",
      "Batch: 6000/83949, Loss: 1.86\n",
      "Batch: 6250/83949, Loss: 2.07\n",
      "Batch: 6500/83949, Loss: 1.63\n",
      "Batch: 6750/83949, Loss: 1.57\n",
      "Batch: 7000/83949, Loss: 2.68\n",
      "Batch: 7250/83949, Loss: 1.74\n",
      "Batch: 7500/83949, Loss: 1.70\n",
      "Batch: 7750/83949, Loss: 1.17\n",
      "Batch: 8000/83949, Loss: 1.87\n",
      "Batch: 8250/83949, Loss: 1.86\n",
      "Batch: 8500/83949, Loss: 2.66\n",
      "Batch: 8750/83949, Loss: 1.64\n",
      "Batch: 9000/83949, Loss: 1.56\n",
      "Batch: 9250/83949, Loss: 1.96\n",
      "Batch: 9500/83949, Loss: 1.39\n",
      "Batch: 9750/83949, Loss: 1.40\n",
      "Batch: 10000/83949, Loss: 1.19\n",
      "Batch: 10250/83949, Loss: 1.91\n",
      "Batch: 10500/83949, Loss: 1.35\n",
      "Batch: 10750/83949, Loss: 1.66\n",
      "Batch: 11000/83949, Loss: 2.05\n",
      "Batch: 11250/83949, Loss: 1.84\n",
      "Batch: 11500/83949, Loss: 1.92\n",
      "Batch: 11750/83949, Loss: 1.21\n",
      "Batch: 12000/83949, Loss: 1.97\n",
      "Batch: 12250/83949, Loss: 1.63\n",
      "Batch: 12500/83949, Loss: 1.39\n",
      "Batch: 12750/83949, Loss: 0.85\n",
      "Batch: 13000/83949, Loss: 1.45\n",
      "Batch: 13250/83949, Loss: 1.09\n",
      "Batch: 13500/83949, Loss: 1.67\n",
      "Batch: 13750/83949, Loss: 1.01\n",
      "Batch: 14000/83949, Loss: 1.80\n",
      "Batch: 14250/83949, Loss: 1.25\n",
      "Batch: 14500/83949, Loss: 1.38\n",
      "Batch: 14750/83949, Loss: 1.33\n",
      "Batch: 15000/83949, Loss: 1.31\n",
      "Batch: 15250/83949, Loss: 1.64\n",
      "Batch: 15500/83949, Loss: 2.16\n",
      "Batch: 15750/83949, Loss: 1.33\n",
      "Batch: 16000/83949, Loss: 1.57\n",
      "Batch: 16250/83949, Loss: 1.62\n",
      "Batch: 16500/83949, Loss: 1.49\n",
      "Batch: 16750/83949, Loss: 1.70\n",
      "Batch: 17000/83949, Loss: 2.16\n",
      "Batch: 17250/83949, Loss: 1.34\n",
      "Batch: 17500/83949, Loss: 1.15\n",
      "Batch: 17750/83949, Loss: 1.87\n",
      "Batch: 18000/83949, Loss: 1.86\n",
      "Batch: 18250/83949, Loss: 1.55\n",
      "Batch: 18500/83949, Loss: 2.22\n",
      "Batch: 18750/83949, Loss: 1.65\n",
      "Batch: 19000/83949, Loss: 1.62\n",
      "Batch: 19250/83949, Loss: 1.31\n",
      "Batch: 19500/83949, Loss: 1.60\n",
      "Batch: 19750/83949, Loss: 1.90\n",
      "Batch: 20000/83949, Loss: 1.35\n",
      "Batch: 20250/83949, Loss: 1.91\n",
      "Batch: 20500/83949, Loss: 2.16\n",
      "Batch: 20750/83949, Loss: 1.84\n",
      "Batch: 21000/83949, Loss: 1.22\n",
      "Batch: 21250/83949, Loss: 1.89\n",
      "Batch: 21500/83949, Loss: 1.80\n",
      "Batch: 21750/83949, Loss: 1.37\n",
      "Batch: 22000/83949, Loss: 1.29\n",
      "Batch: 22250/83949, Loss: 1.61\n",
      "Batch: 22500/83949, Loss: 1.93\n",
      "Batch: 22750/83949, Loss: 1.29\n",
      "Batch: 23000/83949, Loss: 1.38\n",
      "Batch: 23250/83949, Loss: 1.66\n",
      "Batch: 23500/83949, Loss: 1.42\n",
      "Batch: 23750/83949, Loss: 1.77\n",
      "Batch: 24000/83949, Loss: 1.92\n",
      "Batch: 24250/83949, Loss: 1.73\n",
      "Batch: 24500/83949, Loss: 1.59\n",
      "Batch: 24750/83949, Loss: 1.63\n",
      "Batch: 25000/83949, Loss: 1.79\n",
      "Batch: 25250/83949, Loss: 1.69\n",
      "Batch: 25500/83949, Loss: 1.84\n",
      "Batch: 25750/83949, Loss: 1.52\n",
      "Batch: 26000/83949, Loss: 1.53\n",
      "Batch: 26250/83949, Loss: 1.12\n",
      "Batch: 26500/83949, Loss: 1.76\n",
      "Batch: 26750/83949, Loss: 1.92\n",
      "Batch: 27000/83949, Loss: 1.66\n",
      "Batch: 27250/83949, Loss: 2.17\n",
      "Batch: 27500/83949, Loss: 1.30\n",
      "Batch: 27750/83949, Loss: 1.44\n",
      "Batch: 28000/83949, Loss: 1.95\n",
      "Batch: 28250/83949, Loss: 1.63\n",
      "Batch: 28500/83949, Loss: 1.48\n",
      "Batch: 28750/83949, Loss: 1.95\n",
      "Batch: 29000/83949, Loss: 1.51\n",
      "Batch: 29250/83949, Loss: 1.49\n",
      "Batch: 29500/83949, Loss: 1.58\n",
      "Batch: 29750/83949, Loss: 1.44\n",
      "Batch: 30000/83949, Loss: 1.50\n",
      "Batch: 30250/83949, Loss: 1.46\n",
      "Batch: 30500/83949, Loss: 1.68\n",
      "Batch: 30750/83949, Loss: 1.63\n",
      "Batch: 31000/83949, Loss: 1.61\n",
      "Batch: 31250/83949, Loss: 1.42\n",
      "Batch: 31500/83949, Loss: 1.79\n",
      "Batch: 31750/83949, Loss: 1.55\n",
      "Batch: 32000/83949, Loss: 1.72\n",
      "Batch: 32250/83949, Loss: 1.66\n",
      "Batch: 32500/83949, Loss: 1.23\n",
      "Batch: 32750/83949, Loss: 2.42\n",
      "Batch: 33000/83949, Loss: 1.48\n",
      "Batch: 33250/83949, Loss: 1.97\n",
      "Batch: 33500/83949, Loss: 1.78\n",
      "Batch: 33750/83949, Loss: 2.00\n",
      "Batch: 34000/83949, Loss: 1.27\n",
      "Batch: 34250/83949, Loss: 1.97\n",
      "Batch: 34500/83949, Loss: 1.83\n",
      "Batch: 34750/83949, Loss: 1.41\n",
      "Batch: 35000/83949, Loss: 2.20\n",
      "Batch: 35250/83949, Loss: 1.58\n",
      "Batch: 35500/83949, Loss: 1.70\n",
      "Batch: 35750/83949, Loss: 1.66\n",
      "Batch: 36000/83949, Loss: 2.13\n",
      "Batch: 36250/83949, Loss: 1.39\n",
      "Batch: 36500/83949, Loss: 2.19\n",
      "Batch: 36750/83949, Loss: 1.73\n",
      "Batch: 37000/83949, Loss: 1.75\n",
      "Batch: 37250/83949, Loss: 1.34\n",
      "Batch: 37500/83949, Loss: 1.61\n",
      "Batch: 37750/83949, Loss: 1.75\n",
      "Batch: 38000/83949, Loss: 1.61\n",
      "Batch: 38250/83949, Loss: 1.65\n",
      "Batch: 38500/83949, Loss: 1.97\n",
      "Batch: 38750/83949, Loss: 1.52\n",
      "Batch: 39000/83949, Loss: 2.01\n",
      "Batch: 39250/83949, Loss: 1.00\n",
      "Batch: 39500/83949, Loss: 2.38\n",
      "Batch: 39750/83949, Loss: 2.50\n",
      "Batch: 40000/83949, Loss: 1.44\n",
      "Batch: 40250/83949, Loss: 1.63\n",
      "Batch: 40500/83949, Loss: 2.05\n",
      "Batch: 40750/83949, Loss: 1.70\n",
      "Batch: 41000/83949, Loss: 1.59\n",
      "Batch: 41250/83949, Loss: 1.45\n",
      "Batch: 41500/83949, Loss: 2.00\n",
      "Batch: 41750/83949, Loss: 1.94\n",
      "Batch: 42000/83949, Loss: 1.59\n",
      "Batch: 42250/83949, Loss: 1.67\n",
      "Batch: 42500/83949, Loss: 2.01\n",
      "Batch: 42750/83949, Loss: 1.39\n",
      "Batch: 43000/83949, Loss: 1.53\n",
      "Batch: 43250/83949, Loss: 1.66\n",
      "Batch: 43500/83949, Loss: 1.13\n",
      "Batch: 43750/83949, Loss: 1.68\n",
      "Batch: 44000/83949, Loss: 1.89\n",
      "Batch: 44250/83949, Loss: 1.79\n",
      "Batch: 44500/83949, Loss: 1.33\n",
      "Batch: 44750/83949, Loss: 1.65\n",
      "Batch: 45000/83949, Loss: 1.74\n",
      "Batch: 45250/83949, Loss: 1.53\n",
      "Batch: 45500/83949, Loss: 2.10\n",
      "Batch: 45750/83949, Loss: 1.31\n",
      "Batch: 46000/83949, Loss: 1.81\n",
      "Batch: 46250/83949, Loss: 1.86\n",
      "Batch: 46500/83949, Loss: 1.47\n",
      "Batch: 46750/83949, Loss: 1.99\n",
      "Batch: 47000/83949, Loss: 1.30\n",
      "Batch: 47250/83949, Loss: 2.01\n",
      "Batch: 47500/83949, Loss: 1.71\n",
      "Batch: 47750/83949, Loss: 1.32\n",
      "Batch: 48000/83949, Loss: 1.97\n",
      "Batch: 48250/83949, Loss: 1.66\n",
      "Batch: 48500/83949, Loss: 1.50\n",
      "Batch: 48750/83949, Loss: 1.61\n",
      "Batch: 49000/83949, Loss: 2.19\n",
      "Batch: 49250/83949, Loss: 1.86\n",
      "Batch: 49500/83949, Loss: 2.02\n",
      "Batch: 49750/83949, Loss: 1.13\n",
      "Batch: 50000/83949, Loss: 2.32\n",
      "Batch: 50250/83949, Loss: 1.47\n",
      "Batch: 50500/83949, Loss: 1.75\n",
      "Batch: 50750/83949, Loss: 1.83\n",
      "Batch: 51000/83949, Loss: 1.90\n",
      "Batch: 51250/83949, Loss: 1.51\n",
      "Batch: 51500/83949, Loss: 1.83\n",
      "Batch: 51750/83949, Loss: 1.72\n",
      "Batch: 52000/83949, Loss: 1.49\n",
      "Batch: 52250/83949, Loss: 1.81\n",
      "Batch: 52500/83949, Loss: 1.38\n",
      "Batch: 52750/83949, Loss: 1.99\n",
      "Batch: 53000/83949, Loss: 1.72\n",
      "Batch: 53250/83949, Loss: 2.20\n",
      "Batch: 53500/83949, Loss: 2.01\n",
      "Batch: 53750/83949, Loss: 1.44\n",
      "Batch: 54000/83949, Loss: 1.82\n",
      "Batch: 54250/83949, Loss: 1.30\n",
      "Batch: 54500/83949, Loss: 2.20\n",
      "Batch: 54750/83949, Loss: 1.41\n",
      "Batch: 55000/83949, Loss: 1.49\n",
      "Batch: 55250/83949, Loss: 1.20\n",
      "Batch: 55500/83949, Loss: 2.17\n",
      "Batch: 55750/83949, Loss: 1.68\n",
      "Batch: 56000/83949, Loss: 1.51\n",
      "Batch: 56250/83949, Loss: 1.54\n",
      "Batch: 56500/83949, Loss: 1.85\n",
      "Batch: 56750/83949, Loss: 1.55\n",
      "Batch: 57000/83949, Loss: 1.83\n",
      "Batch: 57250/83949, Loss: 1.99\n",
      "Batch: 57500/83949, Loss: 2.18\n",
      "Batch: 57750/83949, Loss: 2.03\n",
      "Batch: 58000/83949, Loss: 1.80\n",
      "Batch: 58250/83949, Loss: 1.82\n",
      "Batch: 58500/83949, Loss: 1.74\n",
      "Batch: 58750/83949, Loss: 1.99\n",
      "Batch: 59000/83949, Loss: 2.00\n",
      "Batch: 59250/83949, Loss: 1.68\n",
      "Batch: 59500/83949, Loss: 1.40\n",
      "Batch: 59750/83949, Loss: 1.92\n",
      "Batch: 60000/83949, Loss: 1.65\n",
      "Batch: 60250/83949, Loss: 1.57\n",
      "Batch: 60500/83949, Loss: 1.84\n",
      "Batch: 60750/83949, Loss: 1.74\n",
      "Batch: 61000/83949, Loss: 1.72\n",
      "Batch: 61250/83949, Loss: 1.29\n",
      "Batch: 61500/83949, Loss: 1.13\n",
      "Batch: 61750/83949, Loss: 1.33\n",
      "Batch: 62000/83949, Loss: 1.99\n",
      "Batch: 62250/83949, Loss: 1.28\n",
      "Batch: 62500/83949, Loss: 1.80\n",
      "Batch: 62750/83949, Loss: 1.36\n",
      "Batch: 63000/83949, Loss: 1.83\n",
      "Batch: 63250/83949, Loss: 1.67\n",
      "Batch: 63500/83949, Loss: 1.59\n",
      "Batch: 63750/83949, Loss: 1.96\n",
      "Batch: 64000/83949, Loss: 1.68\n",
      "Batch: 64250/83949, Loss: 1.41\n",
      "Batch: 64500/83949, Loss: 1.55\n",
      "Batch: 64750/83949, Loss: 1.63\n",
      "Batch: 65000/83949, Loss: 1.71\n",
      "Batch: 65250/83949, Loss: 1.24\n",
      "Batch: 65500/83949, Loss: 1.75\n",
      "Batch: 65750/83949, Loss: 1.48\n",
      "Batch: 66000/83949, Loss: 1.60\n",
      "Batch: 66250/83949, Loss: 1.47\n",
      "Batch: 66500/83949, Loss: 1.79\n",
      "Batch: 66750/83949, Loss: 1.65\n",
      "Batch: 67000/83949, Loss: 1.80\n",
      "Batch: 67250/83949, Loss: 2.24\n",
      "Batch: 67500/83949, Loss: 2.06\n",
      "Batch: 67750/83949, Loss: 1.70\n",
      "Batch: 68000/83949, Loss: 1.75\n",
      "Batch: 68250/83949, Loss: 1.60\n",
      "Batch: 68500/83949, Loss: 1.30\n",
      "Batch: 68750/83949, Loss: 1.54\n",
      "Batch: 69000/83949, Loss: 1.71\n",
      "Batch: 69250/83949, Loss: 2.05\n",
      "Batch: 69500/83949, Loss: 1.66\n",
      "Batch: 69750/83949, Loss: 1.66\n",
      "Batch: 70000/83949, Loss: 1.77\n",
      "Batch: 70250/83949, Loss: 1.36\n",
      "Batch: 70500/83949, Loss: 1.80\n",
      "Batch: 70750/83949, Loss: 1.82\n",
      "Batch: 71000/83949, Loss: 1.17\n",
      "Batch: 71250/83949, Loss: 1.26\n",
      "Batch: 71500/83949, Loss: 1.69\n",
      "Batch: 71750/83949, Loss: 2.11\n",
      "Batch: 72000/83949, Loss: 1.70\n",
      "Batch: 72250/83949, Loss: 1.22\n",
      "Batch: 72500/83949, Loss: 1.53\n",
      "Batch: 72750/83949, Loss: 1.81\n",
      "Batch: 73000/83949, Loss: 1.72\n",
      "Batch: 73250/83949, Loss: 1.59\n",
      "Batch: 73500/83949, Loss: 1.21\n",
      "Batch: 73750/83949, Loss: 1.56\n",
      "Batch: 74000/83949, Loss: 1.86\n",
      "Batch: 74250/83949, Loss: 1.74\n",
      "Batch: 74500/83949, Loss: 1.92\n",
      "Batch: 74750/83949, Loss: 1.72\n",
      "Batch: 75000/83949, Loss: 1.48\n",
      "Batch: 75250/83949, Loss: 1.34\n",
      "Batch: 75500/83949, Loss: 1.28\n",
      "Batch: 75750/83949, Loss: 1.18\n",
      "Batch: 76000/83949, Loss: 1.67\n",
      "Batch: 76250/83949, Loss: 1.59\n",
      "Batch: 76500/83949, Loss: 1.24\n",
      "Batch: 76750/83949, Loss: 1.68\n",
      "Batch: 77000/83949, Loss: 2.02\n",
      "Batch: 77250/83949, Loss: 1.86\n",
      "Batch: 77500/83949, Loss: 1.78\n",
      "Batch: 77750/83949, Loss: 2.05\n",
      "Batch: 78000/83949, Loss: 1.56\n",
      "Batch: 78250/83949, Loss: 2.43\n",
      "Batch: 78500/83949, Loss: 1.94\n",
      "Batch: 78750/83949, Loss: 1.54\n",
      "Batch: 79000/83949, Loss: 1.55\n",
      "Batch: 79250/83949, Loss: 1.33\n",
      "Batch: 79500/83949, Loss: 1.09\n",
      "Batch: 79750/83949, Loss: 1.29\n",
      "Batch: 80000/83949, Loss: 1.48\n",
      "Batch: 80250/83949, Loss: 1.19\n",
      "Batch: 80500/83949, Loss: 1.95\n",
      "Batch: 80750/83949, Loss: 1.58\n",
      "Batch: 81000/83949, Loss: 1.68\n",
      "Batch: 81250/83949, Loss: 2.38\n",
      "Batch: 81500/83949, Loss: 1.45\n",
      "Batch: 81750/83949, Loss: 1.86\n",
      "Batch: 82000/83949, Loss: 1.63\n",
      "Batch: 82250/83949, Loss: 2.07\n",
      "Batch: 82500/83949, Loss: 1.59\n",
      "Batch: 82750/83949, Loss: 1.58\n",
      "Batch: 83000/83949, Loss: 1.29\n",
      "Batch: 83250/83949, Loss: 2.21\n",
      "Batch: 83500/83949, Loss: 1.52\n",
      "Batch: 83750/83949, Loss: 1.96\n",
      "Average training loss: 1.63\n",
      "Validation Accuracy: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [34:51:48<11:36:45, 41805.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/83949, Loss: 1.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [34:52:49<11:37:36, 41856.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Furkan\\Desktop\\MBTI_Final\\better\\model_train_for_2nd.ipynb Cell 3\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Furkan/Desktop/MBTI_Final/better/model_train_for_2nd.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(b_input_ids, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39mb_input_mask, labels\u001b[39m=\u001b[39mb_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Furkan/Desktop/MBTI_Final/better/model_train_for_2nd.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Furkan/Desktop/MBTI_Final/better/model_train_for_2nd.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Furkan/Desktop/MBTI_Final/better/model_train_for_2nd.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Furkan/Desktop/MBTI_Final/better/model_train_for_2nd.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the previously saved model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 16)\n",
    "model.load_state_dict(torch.load(\"bert_model2.pt\"))  # Load the state dict from the saved model\n",
    "model.to(device)\n",
    "\n",
    "# Optimization setup\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*3) # assuming 3 epochs\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if step % 250 == 0:\n",
    "            print(\"Batch: {0}/{1}, Loss: {2:.2f}\".format(step, len(train_dataloader), loss.item()))\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    torch.save(model.state_dict(), f'checkpoint3_{_}.pt') # Save the model checkpoint\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'bert_model3.pt')\n",
    "\n",
    "def evaluate_test_set(test_dataloader):\n",
    "    # Put model in evaluation mode to evaluate loss on the test set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Test Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# Call the function to evaluate the test set\n",
    "evaluate_test_set(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
